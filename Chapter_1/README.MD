### Normalization (정규화)
---
#### 정규화의 정의
- 데이터의 각 특성을 동일한 범위나 분포로 변환하여 데이터 간의 비교를 용이하게 하고, 알고리즘의 성능을 향상시키기 위해서 정규화를 사용합니다.
- 지금까지 훈련 세트와 테스트 세트를 전처리 했던 과정이 기억나시나요?
- trian_input에 대해서 학습을 시킨 후, test_input은 변환만 시켜주었습니다.
- test_input은 왜 학습 시키지 않고 그냥 변환만 시켜주는 걸까요?
---
##### 정규화를 하는 이유
train_input = [1, 2, 3, 4, 5]
test_input = [10, 11, 12]으로 주어져있다고 합시다. train_input의 분포와 test_input의 분포는 당연히 다르죠?
평균은 물론, 분산, 표준편차의 값도 다릅니다.
하지만 우리는 훈련 세트에 대해서 모델을 학습시켜야 합니다.
그래서 훈련 세트로 해당 모델을 학습시키되, 훈련 세트의 평균, 분산, 표준 편차에 맞추어 테스트 세트도 정규화를 진행하는 것 입니다.
만약 따로, 따로 정규화를 진행한다면 평균, 분산, 표준편차 값이 달라서 데이터의 상관성이 없겠죠?
---
#### 정규화 방법
1. Z-점수 정규화 (Z-score Normalization)
- Z-점수 정규화는 데이터의 평균을 0, 표준편차를 1로 변환합니다.
- 평균값을 0으로 설정하는 이유는 해당 값을 정중앙에 놓기 위함입니다. 가운데가 0이면 좌우로 음수, 양수가 대칭이니깐요.
- 만약 평균을 1000을 기준으로 한다면 그래프를 보기 불편할 겁니다.
- 표준편차를 1로 만드는 이유는 데이터의 스케일을 통일시키기 위해 표준편차를 1로 설정합니다. 이는 데이터의 변동성을 유지하되, 상대적인 위치를 비교하기 쉽게 만듭니다.

$X_{standardized}=\frac{X-μ}{σ}$

- μ는 평균, σ는 표준편차입니다.

2. 최소-최대 정규화 (Min-Max Normalization)
- 최소-최대 정규화는 데이터를 0과 1 사이의 범위로 변환합니다.
- 모든 특성의 값을 동일한 범위로 조정하여 상대적인 크기를 유지하는데 이상치에 민감합니다.
- 예를 들어서 a = [1, 2, 3, 4, 5, 6, 1000]인 경우 정규화된 값이 한쪽으로 기울겠죠?

$X_{normalized} = \frac{X-X_{min}}{X_{max}-X_{min}}$

#### 정규분포를 따르는가?
- 모든 데이터가 정규분포를 따를까요? 아님 나름에 규칙이 있을까요?
- 데이터가 정규분포를 따르는지 확인하는 방법은 크게 시각적 방법과 통계적 방법으로 나뉩니다.
##### 시각적 방법
1. 히스토그램 : 데이터를 히스토그램으로 시각화하여 종 모양(정규 분포)에 가까운지를 확인합니다.
<사진>
2. QQ 플롯(Q-Q Plot) : 관측된 데이터의 분위수(quantile)를 정규 분포의 분위수와 비교하는 그래프입니다. 데이터가 정규 분포를 따르기 때문에 직선에 가까운 형태를 보여야 합니다.
<사진>
##### 통계적 검정
- 통계적 검정 방법들을 들어가기 전에 귀무가설, 대립가설에 대해서 알아야합니다.
- 어려워보이지만 사실상 하나도 어렵지 않습니다.
1. 귀무가설과 대립가설
- 한가지 예시를 들어보죠.경찰과 도둑이 있다고 가성합시다. 대부분 경찰서에 오면 도둑들은 뭐라고 하나요? 저는 죄가 없습니다. 라고 주장하죠? 이게 바로 귀무가설입니다. 이에 대해서 우리 경찰들은 뭐라고 답하나요? 당신은 죄가 있습니다! 라고 주장하죠? 이건 대립가설입니다. 두 가설은 대칭을 이루는 걸 기억하세요.
1.1 제1종오류, 제2종오류
- 경찰도 실수를 하겠죠? 경찰이 실수할 수 있는 경우는 총 2가지 입니다. 죄가 없는 사람을 죄인으로 만들거나, 죄가 있는 사람을 죄가 없는 사람을 만들거나 둘중 하나겠죠. 그 둘 다 심각하지만 죄가 없는 사람을 죄인으로 만드는 경우를 제1종오류라고 합니다. 제2종오류는 죄가 있는 사람을 죄가  없는 사람으로 만드는 거겠죠?
1.2 유의수준, 유의 확률
- 유의수준과 유의확률 딱 봐도 어려워보이는 단어입니다. 유의 수준은 통상 0.05나 0.01을 주로 사용한다는 것만 아시면 되는데 제1종오류를 발생시킬 확률을 유의확률이라고 하고, 유의확률이 유의수준을 넘어갔을 경우 해당 가설은 기각되지 않습니다.
- 다시 풀어서 설명하면 도둑이 무죄라고 주장했고(귀무가설) 무죄가 아니다(대립가설)가 틀릴 확률(제1종오류)를 p_value 유의 확률이라고 이야기합니다. p_value가 0.05(도둑이 무죄라고 주장했고 실제로 무죄일 확률)를 넘지 않으면 경찰은 도둑의 의견을 무시(기각)합니다. 반대로 p_value의 값이 0.05를 넘어서는 경우 경찰은 도둑의 이견을 받아들이겠죠.
---
1. Shapiro-Wilk 검정
- 유의수준, 유의확률을 배웠던 이유는 Shapiro-Wilk 검정을 비롯한 모든 통계검정에서 필요하기 때문입니다.
- 검정과정
	1. 데이터 정렬: 주어진 데이터 샘플을 오름차순으로 정렬합니다
	2. 기대값 및 공분산 행렬 계산: 샘플 크기 n에 따라 기대값과 공분산 행렬을 계산합니다. 이때 기대값은 정규 분포에서 예상되는 분위수를 기반으로 계산됩니다.
	3. 검정 통계량 (W) 계산: W 통계량은 다음과 같이 계산됩니다
- 적용 예시
    - 보통 데이터가 소수일 때 정규성을 검정하는 데 유용합니다. 큰 샘플일 경우에는 다른 검정 방법, 예를 들어 Kolmogorov-Smirnov 검정이나 Anderson-Darling 검정 등을 사용할 수 있습니다.
- 한계
    - 샘플 크기가 너무 크면 민감도가 높아져 작은 차이에도 귀무가설을 기각할 수 있습니다.
    - 데이터가 정규분포를 따르지 않는 경우라도, 분포가 크게 왜곡되지 않으면 실무에서 정규분포 가정을 유지해도 무방한 경우가 많습니다.

$W=\frac{(\sum_{i=1}^{n}a_ix_i)^2}{\sum_{i=1}^{n}(x_i-\bar{x})^2}$

<사진>
2. Kolmogorov-Smirnov 검정
- 정의 : 두 분포가 동일한지, 또는 주어진 데이터가 특정 분포(주로 정규분포)를 따르는지를 검정하는 비모수적 방법입니다.
- Kolmogorov-Smirnov 검정의 목적
    - 1-표본 KS 검정: 주어진 데이터가 특정 분포(예: 정규분포)를 따르는지 검정
    - 2-표본 KS 검정: 두 개의 데이터 집합이 동일한 분포를 따르는지 검정.
- 검정 통계량 (D)
    - 정의 : Kolmogorov-Smirnov 검정에서의 검정 통계량 D는 누적분포함수(CDF) 간의 최대 차이로 정의됩니다.
    - F(x): 실제 데이터의 경험적 누적분포함수(ECDF).
    - G(x): 가정된 누적분포함수(CDF) 또는 두 번째 데이터셋의 누적분포함수.

$D=\underset{x}{sup}\left|F_n(x)-F(x) \right|$

<사진>
3. Anderson-Darling 검정
- 정규성 검정의 한 종류로, Kolmogorov-Smirnov 검정보다 더 민감하게 분포의 꼬리 부분에서 차이를 감지할 수 있는 장점. 비모수적 방법입니다.
- 검정 통계량 (A²)
- 주어진 데이터의 경험적 누적분포함수(ECDF)와 가정된 분포의 누적분포함수(CDF) 사이의 차이를 기반으로 합니다. KS 검정과 유사하지만, 안데르손-달링 검정은 데이터 분포의 꼬리 부분에서 차이에 더 가중치를 둡니다.

$A^2=-n-\frac{1}{n} \sum_{i=1}^{n}((2i-1)[lnF(Y_i)+ln(1-F(Y_{n+1-i}))])$
<사진>
